"""RSS Feed Collector"""

import json
import xml.etree.ElementTree as ET
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta

import feedparser

from config.paths import OPML_FILE, ARTICLES_FILE


@dataclass
class Article:
    title: str
    link: str
    summary: str
    source: str
    published: str


def parse_opml(opml_path) -> list[dict]:
    """OPML íŒŒì¼ì—ì„œ RSS í”¼ë“œ URL ì¶”ì¶œ"""
    tree = ET.parse(opml_path)
    feeds = []
    
    for outline in tree.iter("outline"):
        url = outline.get("xmlUrl")
        if url:
            feeds.append({
                "name": outline.get("text", "Unknown"),
                "url": url
            })
    
    return feeds


def get_collection_period() -> tuple[datetime, datetime]:
    """ì§€ë‚œ ì£¼ ì›”ìš”ì¼-ì¼ìš”ì¼ ê¸°ê°„ ê³„ì‚°
    
    ì˜ˆì‹œ (ì˜¤ëŠ˜ = 11/28 ê¸ˆìš”ì¼):
    - today.weekday() = 4 (ê¸ˆìš”ì¼)
    - last_monday = 11/28 - 11 = 11/17
    - last_sunday = 11/17 + 6 = 11/23
    """
    today = datetime.now()
    last_monday = today - timedelta(days=today.weekday() + 7)
    last_sunday = last_monday + timedelta(days=6)
    # ì¼ìš”ì¼ 23:59:59ê¹Œì§€ í¬í•¨
    last_sunday = last_sunday.replace(hour=23, minute=59, second=59)
    return last_monday, last_sunday


def fetch_feed(url: str, name: str, start_date: datetime, end_date: datetime) -> list[Article]:
    """ë‹¨ì¼ RSS í”¼ë“œì—ì„œ ì§€ì • ê¸°ê°„ ê¸°ì‚¬ ìˆ˜ì§‘"""
    articles = []
    
    try:
        feed = feedparser.parse(url)
        
        for entry in feed.entries[:20]:
            # ë°œí–‰ì¼ íŒŒì‹±
            pub_date = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                pub_date = datetime(*entry.published_parsed[:6])
            
            # ê¸°ê°„ ë‚´ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘
            if pub_date and start_date <= pub_date <= end_date:
                summary = getattr(entry, "summary", "")[:500]
                articles.append(Article(
                    title=entry.get("title", ""),
                    link=entry.get("link", ""),
                    summary=summary,
                    source=name,
                    published=pub_date.strftime("%Y-%m-%d")
                ))
    except Exception as e:
        print(f"  âš ï¸ {name}: {e}")
    
    return articles


def collect_all() -> list[dict]:
    """ëª¨ë“  í”¼ë“œì—ì„œ ì§€ë‚œ ì£¼ ê¸°ì‚¬ ìˆ˜ì§‘"""
    feeds = parse_opml(OPML_FILE)
    all_articles = []
    
    start_date, end_date = get_collection_period()
    print(f"ğŸ“¡ {len(feeds)}ê°œ í”¼ë“œì—ì„œ ìˆ˜ì§‘ ì¤‘...")
    print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
    
    for feed in feeds:
        articles = fetch_feed(feed["url"], feed["name"], start_date, end_date)
        all_articles.extend(articles)
        if articles:
            print(f"  âœ“ {feed['name']}: {len(articles)}ê°œ")
    
    # ìµœì‹ ìˆœ ì •ë ¬
    all_articles.sort(key=lambda x: x.published, reverse=True)
    print(f"\nğŸ“° ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    return [asdict(a) for a in all_articles]


def save_articles(articles: list[dict]):
    """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    with open(ARTICLES_FILE, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)


if __name__ == "__main__":
    articles = collect_all()
    save_articles(articles)